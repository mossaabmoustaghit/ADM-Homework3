{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rossella\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import nltk\n",
    "import csv\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection\n",
    "#### For this homework, there is no provided dataset, instead you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of animes\n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357d4bf0b2cf4a468d5cc50b04b40367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "cont = 0\n",
    "for cont in tqdm(range(5001,19130,50)):\n",
    "    URL = f\"https://myanimelist.net/topanime.php?limit={cont}\"\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"div\",{\"class\":\"di-ib clearfix\"})\n",
    "    for link in results:\n",
    "        links.append(link.find(\"a\")[\"href\"])\n",
    "pd.Series(links).to_csv(\"List_of_Anime_s_URLs.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(\"List_of_Anime_s_URLs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c4d5a69dc54d0eba93dc9fd86427e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19138.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed : 0  pages\n",
      "processed : 1  pages\n",
      "processed : 2  pages\n",
      "processed : 3  pages\n",
      "processed : 4  pages\n",
      "processed : 5  pages\n",
      "processed : 6  pages\n",
      "processed : 7  pages\n",
      "processed : 8  pages\n",
      "processed : 9  pages\n",
      "processed : 10  pages\n",
      "processed : 11  pages\n",
      "processed : 12  pages\n",
      "processed : 13  pages\n",
      "processed : 14  pages\n",
      "processed : 15  pages\n",
      "processed : 16  pages\n",
      "processed : 17  pages\n",
      "processed : 18  pages\n",
      "processed : 19  pages\n",
      "processed : 20  pages\n",
      "processed : 21  pages\n",
      "processed : 22  pages\n",
      "processed : 23  pages\n",
      "processed : 24  pages\n",
      "processed : 25  pages\n",
      "processed : 26  pages\n",
      "processed : 27  pages\n",
      "processed : 28  pages\n",
      "processed : 29  pages\n",
      "processed : 30  pages\n",
      "processed : 31  pages\n",
      "processed : 32  pages\n",
      "processed : 33  pages\n",
      "processed : 34  pages\n",
      "processed : 35  pages\n",
      "processed : 36  pages\n",
      "processed : 37  pages\n",
      "processed : 38  pages\n",
      "processed : 39  pages\n",
      "processed : 40  pages\n",
      "processed : 41  pages\n",
      "processed : 42  pages\n",
      "processed : 43  pages\n",
      "processed : 44  pages\n",
      "processed : 45  pages\n",
      "processed : 46  pages\n",
      "processed : 47  pages\n",
      "processed : 48  pages\n",
      "processed : 49  pages\n",
      "processed : 50  pages\n",
      "processed : 51  pages\n",
      "processed : 52  pages\n",
      "processed : 53  pages\n",
      "processed : 54  pages\n",
      "processed : 55  pages\n",
      "processed : 56  pages\n",
      "processed : 57  pages\n",
      "processed : 58  pages\n",
      "processed : 59  pages\n",
      "processed : 60  pages\n",
      "processed : 61  pages\n",
      "processed : 62  pages\n",
      "processed : 63  pages\n",
      "processed : 64  pages\n",
      "processed : 65  pages\n",
      "processed : 66  pages\n",
      "processed : 67  pages\n",
      "processed : 68  pages\n",
      "processed : 69  pages\n",
      "processed : 70  pages\n",
      "processed : 71  pages\n",
      "processed : 72  pages\n",
      "processed : 73  pages\n",
      "processed : 74  pages\n",
      "processed : 75  pages\n",
      "processed : 76  pages\n",
      "processed : 77  pages\n",
      "processed : 78  pages\n",
      "processed : 79  pages\n",
      "processed : 80  pages\n",
      "processed : 81  pages\n",
      "processed : 82  pages\n",
      "processed : 83  pages\n",
      "processed : 84  pages\n",
      "processed : 85  pages\n",
      "processed : 86  pages\n",
      "processed : 87  pages\n",
      "processed : 88  pages\n",
      "processed : 89  pages\n",
      "processed : 90  pages\n",
      "processed : 91  pages\n",
      "processed : 92  pages\n",
      "processed : 93  pages\n",
      "processed : 94  pages\n",
      "processed : 95  pages\n",
      "processed : 96  pages\n",
      "processed : 97  pages\n",
      "processed : 98  pages\n",
      "processed : 99  pages\n",
      "processed : 100  pages\n",
      "processed : 101  pages\n",
      "processed : 102  pages\n",
      "processed : 103  pages\n",
      "processed : 104  pages\n",
      "processed : 105  pages\n",
      "processed : 106  pages\n",
      "processed : 107  pages\n",
      "processed : 108  pages\n",
      "processed : 109  pages\n",
      "processed : 110  pages\n",
      "processed : 111  pages\n",
      "processed : 112  pages\n",
      "processed : 113  pages\n",
      "processed : 114  pages\n",
      "processed : 115  pages\n",
      "processed : 116  pages\n",
      "processed : 117  pages\n",
      "processed : 118  pages\n",
      "processed : 119  pages\n",
      "processed : 120  pages\n",
      "processed : 121  pages\n",
      "processed : 122  pages\n",
      "processed : 123  pages\n",
      "processed : 124  pages\n",
      "processed : 125  pages\n",
      "processed : 126  pages\n",
      "processed : 127  pages\n",
      "processed : 128  pages\n",
      "processed : 129  pages\n",
      "processed : 130  pages\n",
      "processed : 131  pages\n",
      "processed : 132  pages\n",
      "processed : 133  pages\n",
      "processed : 134  pages\n",
      "processed : 135  pages\n",
      "processed : 136  pages\n",
      "processed : 137  pages\n",
      "processed : 138  pages\n",
      "processed : 139  pages\n",
      "processed : 140  pages\n",
      "processed : 141  pages\n",
      "processed : 142  pages\n",
      "processed : 143  pages\n",
      "processed : 144  pages\n",
      "processed : 145  pages\n",
      "processed : 146  pages\n",
      "processed : 147  pages\n",
      "processed : 148  pages\n",
      "processed : 149  pages\n",
      "processed : 150  pages\n",
      "processed : 151  pages\n",
      "processed : 152  pages\n",
      "processed : 153  pages\n",
      "processed : 154  pages\n",
      "processed : 155  pages\n",
      "processed : 156  pages\n",
      "processed : 157  pages\n",
      "processed : 158  pages\n",
      "processed : 159  pages\n",
      "processed : 160  pages\n",
      "processed : 161  pages\n",
      "processed : 162  pages\n",
      "processed : 163  pages\n",
      "processed : 164  pages\n",
      "processed : 165  pages\n",
      "processed : 166  pages\n",
      "processed : 167  pages\n",
      "processed : 168  pages\n",
      "processed : 169  pages\n",
      "processed : 170  pages\n",
      "processed : 171  pages\n",
      "processed : 172  pages\n",
      "processed : 173  pages\n",
      "processed : 174  pages\n",
      "processed : 175  pages\n",
      "processed : 176  pages\n",
      "processed : 177  pages\n",
      "processed : 178  pages\n",
      "processed : 179  pages\n",
      "processed : 180  pages\n",
      "processed : 181  pages\n",
      "9095\n",
      "processed : 182  pages\n",
      "processed : 183  pages\n",
      "processed : 184  pages\n",
      "processed : 185  pages\n",
      "processed : 186  pages\n",
      "processed : 187  pages\n",
      "processed : 188  pages\n",
      "processed : 189  pages\n",
      "processed : 190  pages\n",
      "processed : 191  pages\n",
      "processed : 192  pages\n",
      "processed : 193  pages\n",
      "processed : 194  pages\n",
      "processed : 195  pages\n",
      "processed : 196  pages\n",
      "processed : 197  pages\n",
      "processed : 198  pages\n",
      "processed : 199  pages\n",
      "processed : 200  pages\n",
      "processed : 201  pages\n",
      "processed : 202  pages\n",
      "processed : 203  pages\n",
      "processed : 204  pages\n",
      "processed : 205  pages\n",
      "processed : 206  pages\n",
      "processed : 207  pages\n",
      "processed : 208  pages\n",
      "processed : 209  pages\n",
      "processed : 210  pages\n",
      "processed : 211  pages\n",
      "processed : 212  pages\n",
      "processed : 213  pages\n",
      "processed : 214  pages\n",
      "processed : 215  pages\n",
      "processed : 216  pages\n",
      "processed : 217  pages\n",
      "processed : 218  pages\n",
      "processed : 219  pages\n",
      "processed : 220  pages\n",
      "processed : 221  pages\n",
      "processed : 222  pages\n",
      "processed : 223  pages\n",
      "processed : 224  pages\n",
      "processed : 225  pages\n",
      "processed : 226  pages\n",
      "processed : 227  pages\n",
      "processed : 228  pages\n",
      "processed : 229  pages\n",
      "processed : 230  pages\n",
      "processed : 231  pages\n",
      "processed : 232  pages\n",
      "processed : 233  pages\n",
      "processed : 234  pages\n",
      "processed : 235  pages\n",
      "processed : 236  pages\n",
      "processed : 237  pages\n",
      "processed : 238  pages\n",
      "processed : 239  pages\n",
      "processed : 240  pages\n",
      "processed : 241  pages\n",
      "processed : 242  pages\n",
      "processed : 243  pages\n",
      "processed : 244  pages\n",
      "processed : 245  pages\n",
      "processed : 246  pages\n",
      "processed : 247  pages\n",
      "processed : 248  pages\n",
      "processed : 249  pages\n",
      "processed : 250  pages\n",
      "processed : 251  pages\n",
      "processed : 252  pages\n",
      "processed : 253  pages\n",
      "processed : 254  pages\n",
      "processed : 255  pages\n",
      "processed : 256  pages\n",
      "processed : 257  pages\n",
      "processed : 258  pages\n",
      "processed : 259  pages\n",
      "processed : 260  pages\n",
      "processed : 261  pages\n",
      "processed : 262  pages\n",
      "processed : 263  pages\n",
      "processed : 264  pages\n",
      "processed : 265  pages\n",
      "processed : 266  pages\n",
      "processed : 267  pages\n",
      "processed : 268  pages\n",
      "processed : 269  pages\n",
      "processed : 270  pages\n",
      "processed : 271  pages\n",
      "processed : 272  pages\n",
      "processed : 273  pages\n",
      "processed : 274  pages\n",
      "processed : 275  pages\n",
      "processed : 276  pages\n",
      "processed : 277  pages\n",
      "processed : 278  pages\n",
      "processed : 279  pages\n",
      "processed : 280  pages\n",
      "processed : 281  pages\n",
      "processed : 282  pages\n",
      "processed : 283  pages\n",
      "processed : 284  pages\n",
      "processed : 285  pages\n",
      "processed : 286  pages\n",
      "processed : 287  pages\n",
      "processed : 288  pages\n",
      "processed : 289  pages\n",
      "processed : 290  pages\n",
      "processed : 291  pages\n",
      "processed : 292  pages\n",
      "processed : 293  pages\n",
      "processed : 294  pages\n",
      "processed : 295  pages\n",
      "processed : 296  pages\n",
      "processed : 297  pages\n",
      "processed : 298  pages\n",
      "processed : 299  pages\n",
      "processed : 300  pages\n",
      "processed : 301  pages\n",
      "processed : 302  pages\n",
      "processed : 303  pages\n",
      "processed : 304  pages\n",
      "processed : 305  pages\n",
      "processed : 306  pages\n",
      "processed : 307  pages\n",
      "processed : 308  pages\n",
      "processed : 309  pages\n",
      "processed : 310  pages\n",
      "processed : 311  pages\n",
      "processed : 312  pages\n",
      "processed : 313  pages\n",
      "processed : 314  pages\n",
      "processed : 315  pages\n",
      "processed : 316  pages\n",
      "processed : 317  pages\n",
      "processed : 318  pages\n",
      "processed : 319  pages\n",
      "processed : 320  pages\n",
      "processed : 321  pages\n",
      "processed : 322  pages\n",
      "processed : 323  pages\n",
      "processed : 324  pages\n",
      "processed : 325  pages\n",
      "processed : 326  pages\n",
      "processed : 327  pages\n",
      "processed : 328  pages\n",
      "processed : 329  pages\n",
      "processed : 330  pages\n",
      "processed : 331  pages\n",
      "processed : 332  pages\n",
      "processed : 333  pages\n",
      "processed : 334  pages\n",
      "processed : 335  pages\n",
      "processed : 336  pages\n",
      "processed : 337  pages\n",
      "processed : 338  pages\n",
      "processed : 339  pages\n",
      "processed : 340  pages\n",
      "processed : 341  pages\n",
      "processed : 342  pages\n",
      "processed : 343  pages\n",
      "processed : 344  pages\n",
      "processed : 345  pages\n",
      "processed : 346  pages\n",
      "processed : 347  pages\n",
      "processed : 348  pages\n",
      "processed : 349  pages\n",
      "processed : 350  pages\n",
      "processed : 351  pages\n",
      "processed : 352  pages\n",
      "processed : 353  pages\n",
      "processed : 354  pages\n",
      "processed : 355  pages\n",
      "processed : 356  pages\n",
      "processed : 357  pages\n",
      "processed : 358  pages\n",
      "processed : 359  pages\n",
      "processed : 360  pages\n",
      "processed : 361  pages\n",
      "processed : 362  pages\n",
      "processed : 363  pages\n",
      "processed : 364  pages\n",
      "processed : 365  pages\n",
      "processed : 366  pages\n",
      "processed : 367  pages\n",
      "processed : 368  pages\n",
      "processed : 369  pages\n",
      "processed : 370  pages\n",
      "processed : 371  pages\n",
      "processed : 372  pages\n",
      "processed : 373  pages\n",
      "processed : 374  pages\n",
      "processed : 375  pages\n",
      "processed : 376  pages\n",
      "processed : 377  pages\n",
      "processed : 378  pages\n",
      "processed : 379  pages\n",
      "processed : 380  pages\n",
      "processed : 381  pages\n",
      "processed : 382  pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(len(links))):\n",
    "    if n % 50 == 0:\n",
    "        print(\"processed :\",n//50,\" pages\")\n",
    "        path = f\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/html/page_{n//50}\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    ua = UserAgent()\n",
    "    header = {'User-Agent':str(ua.random)}\n",
    "    page = requests.get(links.values[n][0],headers=header)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    button = soup.find(\"button\",{\"class\":\"g-recaptcha\"})\n",
    "    if button:\n",
    "        print(\"è accaduto\")\n",
    "        browser = webdriver.Chrome(\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/chromedriver\")\n",
    "        browser.get(links.values[n][0])\n",
    "        browser.find_element_by_class_name('g-recaptcha').click()\n",
    "        time.sleep(100)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    with open(path + f\"/article_{n}.html\", \"w\",encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "#### At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Anime Name (to save as animeTitle): String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd29f7023eb441e8ab438b14081b3c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for folder in tqdm(glob.glob(\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/html/page*\")):\n",
    "    for html in glob.glob(f\"{folder}/*.html\"):\n",
    "        soup_i = BeautifulSoup(open(html,\"r\", encoding=\"utf-8\"), 'html.parser') #give path to bs4\n",
    "        try :\n",
    "            animeTitle = soup_i.find(\"h1\", {\"class\":\"title-name h1_bold_none\"}).text.strip()\n",
    "        except: \n",
    "            animeTitle = \"\" \n",
    "        try:\n",
    "            animeType = str(soup_i.find(\"span\",text=\"Type:\").parent.a.text)\n",
    "        except:\n",
    "            animeType = \"\"\n",
    "        try:\n",
    "            animeNumEpisode = int(soup_i.find(\"span\",text=\"Episodes:\").parent.text.replace('Episodes:' , '').strip())\n",
    "        except:\n",
    "            animeNumEpisode = \"\"\n",
    "        try:\n",
    "            releaseDate =datetime.strptime(str(soup_i.find(\"span\",text=\"Aired:\").parent.text.replace('Aired:' , '').strip().split(' to ')[0]),'%b %d, %Y').date()\n",
    "        except:\n",
    "            releaseDate = \"\"\n",
    "        try:\n",
    "            endDate = datetime.strptime(str(soup_i.find(\"span\",text=\"Aired:\").parent.text.replace('Aired:' , '').strip().split(' to ')[1]),'%b %d, %Y').date()\n",
    "        except:\n",
    "            endDate = \"\"\n",
    "        try:\n",
    "            animeNumMembers = int(soup_i.find(\"span\",text=\"Members:\").parent.text.replace(\"Members:\",\"\").replace(\",\",\"\").strip())\n",
    "        except:\n",
    "            animeNumMembers = \"\"\n",
    "        try :\n",
    "            animeScore = float(soup_i.find(\"div\", {\"class\":\"score-label\"}).text.strip())\n",
    "        except: \n",
    "            animeScore = \"\" \n",
    "        try :\n",
    "            animeUsers = int(soup_i.find(\"div\",{\"class\":\"fl-l score\"})[\"data-user\"].replace(',','').replace(' users',''))\n",
    "        except: \n",
    "            animeUsers = \"\"  \n",
    "        try :\n",
    "            animeRank = int(soup_i.find(\"span\",{\"class\":\"numbers ranked\"}).text.strip().replace(\"Ranked\",\"\").replace('#',''))\n",
    "        except: \n",
    "            animeRank = \"\" \n",
    "        try :\n",
    "            animePopularity = int(soup_i.find(\"span\",{\"class\":\"numbers popularity\"}).text.strip().replace(\"Popularity\",\"\").replace('#',''))\n",
    "        except: \n",
    "            animePopularity = \"\" \n",
    "        try :\n",
    "            animeDescription = str(soup_i.find(\"p\",{\"itemprop\":\"description\"}).text.strip()).replace(\"\\n\",\"\")\n",
    "        except: \n",
    "            animeDescription = \"\"\n",
    "        try:\n",
    "            animeRelated = list(set([tag.parent.a[\"href\"].split(\"/\")[-1].strip() for tag in soup_i.find(\"table\",{\"class\":\"anime_detail_related_anime\"}).find_all(\"td\",{\"class\":\"borderClass\"})]))\n",
    "        except:\n",
    "            animeRelated = []\n",
    "        try:\n",
    "            animeCharacters = [tag.a.text for tag in soup_i.find_all(\"h3\",{\"class\":\"h3_characters_voice_actors\"})]\n",
    "        except:\n",
    "            animeCharacters = []\n",
    "        try:\n",
    "            animeVoices = [voice.a.text for voice in soup_i.find_all(\"td\",{\"class\":\"va-t ar pl4 pr4\"})]\n",
    "        except:\n",
    "            animeVoices = []\n",
    "        try:\n",
    "            for staff in all_staff.find_all(\"tr\"):\n",
    "                staff_name = staff.find_all(\"td\", {\"class\":\"borderClass\"})[1].a.text.strip()\n",
    "                staff_task = staff.find_all(\"td\", {\"class\":\"borderClass\"})[1].div.text.strip()\n",
    "                animeStaff.append([staff_name,staff_task])\n",
    "        except:\n",
    "            animeStaff = []\n",
    "     \n",
    "        info = [(animeTitle, animeType,animeNumEpisode,releaseDate,endDate,animeNumMembers,animeScore,animeUsers,animeRank,animePopularity,animeDescription,animeRelated,animeCharacters,animeVoices,animeStaff)]\n",
    "        info = pd.DataFrame(info, columns = [\"animeTitle\", \"animeType\",\"animeNumEpisode\",\"releaseDate\",\"endDate\",\"animeNumMembers\",\"animeScore\",\"animeUsers\",\"animeRank\",\"animePopularity\",\"animeDescription\",\"animeRelated\",\"animeCharacters\",\"animeVoices\",\"animeStaff\"])\n",
    "        path = \"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/AnimeTsv1/\"\n",
    "        info.to_csv(path + \"anime_\" + re.findall(\"[0-9]+\",html)[-1]+\".tsv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Search Engine\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "Removing stopwords\n",
    "Removing punctuation\n",
    "Stemming\n",
    "Anything else you think it's needed\n",
    "For this purpose, you can use the nltk library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(phrase):\n",
    "    try:\n",
    "        phrase = phrase.lower()  # Converting to lowercase\n",
    "        phrase = re.sub(r'[.|,|)|(|\\|/|?|!|\\'|\"|#|-]', r' ', phrase)\n",
    "        phrase = \" \".join(re.findall(r\"(?i)\\b[a-z]+\\b\", phrase))\n",
    "        return \" \".join([lemmatizer.lemmatize(token.strip()) for token in word_tokenize(phrase)\n",
    "                     if token not in stop_words and len(token) > 1])\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous algorithm takes care of simultaneously performing the operations of stemming, lemmatization, removal of punctuation marks, conversion of text into lowercase, removal of numbers. It also consider all words that have a minimum length greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "df = pd.concat([pd.read_csv(folder) for folder in glob.glob(\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/AnimeTsv1/*.tsv\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df.drop(columns = \"Unnamed: 0\")\n",
    "df.reset_index(drop=True).to_csv(\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/final_df.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Rossella/OneDrive/Desktop/ADM/HW3/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdef8a814bb44def82784e45c7943e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14135.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"animeDescription\"] = df[\"animeDescription\"].fillna(\" \") \n",
    "df[\"animeDescription_new\"] = df.animeDescription.progress_apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description.\n",
    "\n",
    "2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    " -- Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider only word that appear more than one time in the documents. This is to avoid words that could come from typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(df[\"animeDescription_new\"])\n",
    "count_df = pd.DataFrame(X.toarray(),index = [f\"document_{i}\" for i in df.index], columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dictionary = {column:count_df[count_df[column] > 0].index.tolist() for column in count_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a_dictionary.pickle', 'wb') as fp:\n",
    "    pickle.dump(a_dictionary, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_dic = pickle.load(open(\"a_dictionary.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "saiyan race\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "- animeTitle\n",
    "- animeDescription\n",
    "- Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_final_result(query,inv_dic):\n",
    "    final_results = []\n",
    "    c = Counter()\n",
    "    for i in query.split():\n",
    "        try:\n",
    "            c.update(inv_dic[i])\n",
    "        except:\n",
    "            pass\n",
    "    for key,value in c.items():\n",
    "        if value == len(query.split()):\n",
    "            final_results.append(key)\n",
    "    return final_results\n",
    "\n",
    "def first_engine(df,inv_dic,col_to_show):\n",
    "    print(\"write here: \")\n",
    "    query = input().strip()\n",
    "    query = clean(query)\n",
    "    c = Counter()\n",
    "    final_result = take_final_result(query,inv_dic)\n",
    "    return df[df[\"index\"].isin(final_result)][col_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write here: \n",
      "pirate\n",
      "pirate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Byeolnala Samchongsa</td>\n",
       "      <td>Three Earth boys aid a space princess to prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Cosmic Fantasy: Ginga Mehyou no Wana</td>\n",
       "      <td>Yuu and Saya are two of the top Cosmic Hunters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Space Travelers The Animation</td>\n",
       "      <td>In the New Cosmic Century 038, humanity is sud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>Nareuneun Dwaeji - Haejeok Mateo</td>\n",
       "      <td>A 2004 Korean CG film about an anthropomorphic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>Noonbory</td>\n",
       "      <td>Having not only a narrator that enhances the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>Jim Button</td>\n",
       "      <td>Jim Button and his best friend Luke live on an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545</th>\n",
       "      <td>Kaizoku Ouji</td>\n",
       "      <td>Kid was brought up on a small island which flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>Uchuu Kaizoku Sara</td>\n",
       "      <td>Sarah is a most terrible pirate knight who dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>Mashou no Nie 3</td>\n",
       "      <td>Long ago, the infamous pirate Van Clad ruled t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>Blood Royale</td>\n",
       "      <td>A perverted pirate sails the seven seas, armed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>Battle Can²</td>\n",
       "      <td>Join this band of femme fatales as they sacrif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>Kyouryuu Boukenki Jura Tripper</td>\n",
       "      <td>On a school yachting trip, fifteen children of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>Can Ci Pin: Fangzhu Xingkong</td>\n",
       "      <td>Year 270 of the Nova Calendar, General Lin of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>Tottoko Hamtarou Movie 3: Ham Ham Grand Prix A...</td>\n",
       "      <td>In Aurora Village in the high frozen peaks of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>Cosmo Warrior Zero</td>\n",
       "      <td>The long war between the planet Earth and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>Gun Frontier</td>\n",
       "      <td>It is a harsh and barren wasteland, where the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10570</th>\n",
       "      <td>Buta</td>\n",
       "      <td>A few days before Fox officially inherits the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10644</th>\n",
       "      <td>Tactical Roar</td>\n",
       "      <td>In the near future the world's climate shifted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>Uchuu Kaizoku Mito no Daibouken</td>\n",
       "      <td>Mito isn't just another space pirate, she's a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>Sol Bianca: Taiyou no Fune</td>\n",
       "      <td>Thousands of years into the future, mankind ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10991</th>\n",
       "      <td>Doubutsu Takarajima</td>\n",
       "      <td>Jim is working as an innkeeper, together with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11080</th>\n",
       "      <td>Last Exile: Ginyoku no Fam Movie - Over the Wi...</td>\n",
       "      <td>A recap film with some new scenes.The Sky Pira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11129</th>\n",
       "      <td>Sol Bianca</td>\n",
       "      <td>Five female pirates pilot the Sol Bianca, a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11704</th>\n",
       "      <td>A-Ko The Versus</td>\n",
       "      <td>A-ko and B-ko are bounty hunters in an untamed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12135</th>\n",
       "      <td>Shining Hearts: Shiawase no Pan</td>\n",
       "      <td>One day, a mysterious girl named Kaguya was wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12138</th>\n",
       "      <td>Uchuu Kaizoku Mito no Daibouken: Futari no Joo...</td>\n",
       "      <td>Aoi was raised as a normal boy on Earth. Littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12159</th>\n",
       "      <td>Teki wa Kaizoku: Neko-tachi no Kyouen</td>\n",
       "      <td>Apulo (a cat-like space alien with the ability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12366</th>\n",
       "      <td>Choujin Locke: Lord Leon</td>\n",
       "      <td>It is the year 336 of the Space Century on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12374</th>\n",
       "      <td>Haja Taisei Dangaiou</td>\n",
       "      <td>Four psychic teenagers—Mia Alice, Roll Kran, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>Beast Wars Second Chou Seimeitai Transformers:...</td>\n",
       "      <td>The movie begins with a space battle between t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505</th>\n",
       "      <td>Dr. Slump Movie 10: Arale no Bikkuriman</td>\n",
       "      <td>Arale and company arrive at a nice location fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12901</th>\n",
       "      <td>Seihou Tenshi Angel Links</td>\n",
       "      <td>Li Meifon is the head of a free, no expenses p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13039</th>\n",
       "      <td>Shin Takarajima</td>\n",
       "      <td>This was the first episode of Mushi Pro Land, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13304</th>\n",
       "      <td>Hoshi Neko Fullhouse</td>\n",
       "      <td>This slapstick comedy is about three pretty gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>Gundam: G no Reconguista</td>\n",
       "      <td>In the year Regild Century 1014, an entire mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13913</th>\n",
       "      <td>100-man-nen Chikyuu no Tabi: Bander Book</td>\n",
       "      <td>To escape the explosion of their ship caused b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              animeTitle  \\\n",
       "358                                 Byeolnala Samchongsa   \n",
       "605                 Cosmic Fantasy: Ginga Mehyou no Wana   \n",
       "893                        Space Travelers The Animation   \n",
       "2320                    Nareuneun Dwaeji - Haejeok Mateo   \n",
       "2461                                            Noonbory   \n",
       "6399                                          Jim Button   \n",
       "6545                                        Kaizoku Ouji   \n",
       "7891                                  Uchuu Kaizoku Sara   \n",
       "8163                                     Mashou no Nie 3   \n",
       "8751                                        Blood Royale   \n",
       "9101                                         Battle Can²   \n",
       "9325                      Kyouryuu Boukenki Jura Tripper   \n",
       "9408                        Can Ci Pin: Fangzhu Xingkong   \n",
       "9753   Tottoko Hamtarou Movie 3: Ham Ham Grand Prix A...   \n",
       "9933                                  Cosmo Warrior Zero   \n",
       "9938                                        Gun Frontier   \n",
       "10570                                               Buta   \n",
       "10644                                      Tactical Roar   \n",
       "10937                    Uchuu Kaizoku Mito no Daibouken   \n",
       "10952                         Sol Bianca: Taiyou no Fune   \n",
       "10991                                Doubutsu Takarajima   \n",
       "11080  Last Exile: Ginyoku no Fam Movie - Over the Wi...   \n",
       "11129                                         Sol Bianca   \n",
       "11704                                    A-Ko The Versus   \n",
       "12135                    Shining Hearts: Shiawase no Pan   \n",
       "12138  Uchuu Kaizoku Mito no Daibouken: Futari no Joo...   \n",
       "12159              Teki wa Kaizoku: Neko-tachi no Kyouen   \n",
       "12366                           Choujin Locke: Lord Leon   \n",
       "12374                               Haja Taisei Dangaiou   \n",
       "12397  Beast Wars Second Chou Seimeitai Transformers:...   \n",
       "12505            Dr. Slump Movie 10: Arale no Bikkuriman   \n",
       "12901                          Seihou Tenshi Angel Links   \n",
       "13039                                    Shin Takarajima   \n",
       "13304                               Hoshi Neko Fullhouse   \n",
       "13656                           Gundam: G no Reconguista   \n",
       "13913           100-man-nen Chikyuu no Tabi: Bander Book   \n",
       "\n",
       "                                        animeDescription  \n",
       "358    Three Earth boys aid a space princess to prote...  \n",
       "605    Yuu and Saya are two of the top Cosmic Hunters...  \n",
       "893    In the New Cosmic Century 038, humanity is sud...  \n",
       "2320   A 2004 Korean CG film about an anthropomorphic...  \n",
       "2461   Having not only a narrator that enhances the e...  \n",
       "6399   Jim Button and his best friend Luke live on an...  \n",
       "6545   Kid was brought up on a small island which flo...  \n",
       "7891   Sarah is a most terrible pirate knight who dan...  \n",
       "8163   Long ago, the infamous pirate Van Clad ruled t...  \n",
       "8751   A perverted pirate sails the seven seas, armed...  \n",
       "9101   Join this band of femme fatales as they sacrif...  \n",
       "9325   On a school yachting trip, fifteen children of...  \n",
       "9408   Year 270 of the Nova Calendar, General Lin of ...  \n",
       "9753   In Aurora Village in the high frozen peaks of ...  \n",
       "9933   The long war between the planet Earth and the ...  \n",
       "9938   It is a harsh and barren wasteland, where the ...  \n",
       "10570  A few days before Fox officially inherits the ...  \n",
       "10644  In the near future the world's climate shifted...  \n",
       "10937  Mito isn't just another space pirate, she's a ...  \n",
       "10952  Thousands of years into the future, mankind ha...  \n",
       "10991  Jim is working as an innkeeper, together with ...  \n",
       "11080  A recap film with some new scenes.The Sky Pira...  \n",
       "11129  Five female pirates pilot the Sol Bianca, a st...  \n",
       "11704  A-ko and B-ko are bounty hunters in an untamed...  \n",
       "12135  One day, a mysterious girl named Kaguya was wa...  \n",
       "12138  Aoi was raised as a normal boy on Earth. Littl...  \n",
       "12159  Apulo (a cat-like space alien with the ability...  \n",
       "12366  It is the year 336 of the Space Century on the...  \n",
       "12374  Four psychic teenagers—Mia Alice, Roll Kran, L...  \n",
       "12397  The movie begins with a space battle between t...  \n",
       "12505  Arale and company arrive at a nice location fo...  \n",
       "12901  Li Meifon is the head of a free, no expenses p...  \n",
       "13039  This was the first episode of Mushi Pro Land, ...  \n",
       "13304  This slapstick comedy is about three pretty gi...  \n",
       "13656  In the year Regild Century 1014, an entire mil...  \n",
       "13913  To escape the explosion of their ship caused b...  "
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"index\"] = [f\"document_{i}\" for i in df.index]\n",
    "first_engine(df,inv_dic,[\"animeTitle\",\"animeDescription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Execute the query\n",
    "In this new setting, given a query you get the right set of documents (i.e., those containing all the words in the query) and sort them according to their similairty to the query. For this purpose, as scoring function we will use the Cosine Similarity with respect to the tfIdf representations of the documents.\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "saiyan race\n",
    "\n",
    "the search engine is supposed to return a list of documents, ranked by their Cosine Similarity with respect to the query entered in input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "- animeTitle\n",
    "- animeDescription\n",
    "- Url\n",
    "- The similarity score of the documents with respect to the query (float value between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_engine(df):\n",
    "    print(\"write here: \")\n",
    "    query = input().strip()\n",
    "    query = clean(query)\n",
    "    vectorizer = TfidfVectorizer(min_df = 2)\n",
    "    trsfm = vectorizer.fit_transform(df[\"animeDescription_new\"])\n",
    "    df[\"similarity\"] = vectorizer.transform([query]).toarray()[0].dot(trsfm.toarray().T).flatten()\n",
    "    return df.sort_values(by = \"similarity\",ascending = False)[[\"animeTitle\",\"animeDescription\",\"similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write here: \n",
      "king pirates\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaizoku Ouji</td>\n",
       "      <td>Kid was brought up on a small island which flo...</td>\n",
       "      <td>0.404249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nareuneun Dwaeji - Haejeok Mateo</td>\n",
       "      <td>A 2004 Korean CG film about an anthropomorphic...</td>\n",
       "      <td>0.337323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uchuu Kaizoku Mito no Daibouken</td>\n",
       "      <td>Mito isn't just another space pirate, she's a ...</td>\n",
       "      <td>0.302232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinkoroheibei Tamatebako</td>\n",
       "      <td>The mischievous Chinkoroheibei visits the unde...</td>\n",
       "      <td>0.286618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mashou no Nie 3</td>\n",
       "      <td>Long ago, the infamous pirate Van Clad ruled t...</td>\n",
       "      <td>0.259308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (1988)</td>\n",
       "      <td>Continuation of the series from 1987.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (2009)</td>\n",
       "      <td>The story of a baby dinosaur and his friends.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (Movie)</td>\n",
       "      <td>The story begins when the frozen Dooly on the ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agukaru: Play with Ibaraki-hen Episode 0</td>\n",
       "      <td>The first half of the episode recaps the first...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Larva Island</td>\n",
       "      <td>The 4th season of Larva. Stranded on a tropica...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14135 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  animeTitle  \\\n",
       "0                               Kaizoku Ouji   \n",
       "0           Nareuneun Dwaeji - Haejeok Mateo   \n",
       "0            Uchuu Kaizoku Mito no Daibouken   \n",
       "0                  Chinkoroheibei Tamatebako   \n",
       "0                            Mashou no Nie 3   \n",
       "..                                       ...   \n",
       "0                 Agigongryong Doolie (1988)   \n",
       "0                 Agigongryong Doolie (2009)   \n",
       "0                Agigongryong Doolie (Movie)   \n",
       "0   Agukaru: Play with Ibaraki-hen Episode 0   \n",
       "0                               Larva Island   \n",
       "\n",
       "                                     animeDescription  similarity  \n",
       "0   Kid was brought up on a small island which flo...    0.404249  \n",
       "0   A 2004 Korean CG film about an anthropomorphic...    0.337323  \n",
       "0   Mito isn't just another space pirate, she's a ...    0.302232  \n",
       "0   The mischievous Chinkoroheibei visits the unde...    0.286618  \n",
       "0   Long ago, the infamous pirate Van Clad ruled t...    0.259308  \n",
       "..                                                ...         ...  \n",
       "0               Continuation of the series from 1987.    0.000000  \n",
       "0       The story of a baby dinosaur and his friends.    0.000000  \n",
       "0   The story begins when the frozen Dooly on the ...    0.000000  \n",
       "0   The first half of the episode recaps the first...    0.000000  \n",
       "0   The 4th season of Larva. Stranded on a tropica...    0.000000  \n",
       "\n",
       "[14135 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_engine(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a new score!\n",
    "Now it's your turn. Build a new metric to rank animes based on the queries of their users.\n",
    "\n",
    "In this scenario, a single user can give in input more information than the single textual query, so you need to take into account all this information, and think a creative and logical way on how to answer at user's requests.\n",
    "\n",
    "Practically:\n",
    "\n",
    "The user will enter you a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 3.1.\n",
    "\n",
    "Once you have the documents, you need to sort them according to your new score. In this step you won't have anymore to take into account just the plot of the documents, you must use the remaining variables in your dataset (or new possible variables that you can create from the existing ones...). You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "\n",
    "N.B.: You have to define a scoring function, not a filter!\n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "- animeTitle\n",
    "- animeDescription\n",
    "- Url\n",
    "\n",
    "The new similarity score of the documents with respect to the query\n",
    "Are the results you obtain better than with the previous scoring function. Explain and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_engine(df):\n",
    "    print(\"write here: \")\n",
    "    query = input().strip()\n",
    "    query = clean(query)\n",
    "    df[\"animeTitle_new\"] = df.animeTitle.progress_apply(clean)\n",
    "    vectorizer = TfidfVectorizer(min_df = 2).fit(df[\"animeDescription_new\"])\n",
    "    query_vec = vectorizer.transform([query]).toarray()[0]\n",
    "    animetitle_sim = query_vec.dot(vectorizer.transform(df[\"animeTitle_new\"]).toarray().T).flatten()\n",
    "    animeDesc_sim = query_vec.dot(vectorizer.transform(df[\"animeDescription_new\"]).toarray().T).flatten()\n",
    "    df[\"similarity\"] = animetitle_sim + animeDesc_sim\n",
    "    return df.sort_values(by = \"similarity\",ascending = False)[[\"animeTitle\",\"animeDescription\",\"similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write here: \n",
      "king pirates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2aff34c9114cd08df7d63212864565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14135.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Was King</td>\n",
       "      <td>A music video for One Ok Rock's song \"I Was Ki...</td>\n",
       "      <td>0.782564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overman King Gainer Recap</td>\n",
       "      <td>Short summary of the first 11 episodes include...</td>\n",
       "      <td>0.598008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kodai Ouja Kyouryuu King</td>\n",
       "      <td>Dr. Taylor was on a paleontology mission with ...</td>\n",
       "      <td>0.593901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The King of Fighters: Destiny</td>\n",
       "      <td>CG anime adaptation of The King of Fighters fi...</td>\n",
       "      <td>0.505139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elec-king The Animation</td>\n",
       "      <td>A surrealist comedy about a local neighborhood...</td>\n",
       "      <td>0.472702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (1988)</td>\n",
       "      <td>Continuation of the series from 1987.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (2009)</td>\n",
       "      <td>The story of a baby dinosaur and his friends.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agigongryong Doolie (Movie)</td>\n",
       "      <td>The story begins when the frozen Dooly on the ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agukaru: Play with Ibaraki-hen Episode 0</td>\n",
       "      <td>The first half of the episode recaps the first...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Larva Island</td>\n",
       "      <td>The 4th season of Larva. Stranded on a tropica...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14135 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  animeTitle  \\\n",
       "0                                 I Was King   \n",
       "0                  Overman King Gainer Recap   \n",
       "0                   Kodai Ouja Kyouryuu King   \n",
       "0              The King of Fighters: Destiny   \n",
       "0                    Elec-king The Animation   \n",
       "..                                       ...   \n",
       "0                 Agigongryong Doolie (1988)   \n",
       "0                 Agigongryong Doolie (2009)   \n",
       "0                Agigongryong Doolie (Movie)   \n",
       "0   Agukaru: Play with Ibaraki-hen Episode 0   \n",
       "0                               Larva Island   \n",
       "\n",
       "                                     animeDescription  similarity  \n",
       "0   A music video for One Ok Rock's song \"I Was Ki...    0.782564  \n",
       "0   Short summary of the first 11 episodes include...    0.598008  \n",
       "0   Dr. Taylor was on a paleontology mission with ...    0.593901  \n",
       "0   CG anime adaptation of The King of Fighters fi...    0.505139  \n",
       "0   A surrealist comedy about a local neighborhood...    0.472702  \n",
       "..                                                ...         ...  \n",
       "0               Continuation of the series from 1987.    0.000000  \n",
       "0       The story of a baby dinosaur and his friends.    0.000000  \n",
       "0   The story begins when the frozen Dooly on the ...    0.000000  \n",
       "0   The first half of the episode recaps the first...    0.000000  \n",
       "0   The 4th season of Larva. Stranded on a tropica...    0.000000  \n",
       "\n",
       "[14135 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_engine(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third engine combines the first and the second engines. In this case the similarity takes an higher value if one or more words inserted in the input box are contained in the Title. In fact inserting in the input box the phrase \"king pirates\", with the second engine we obtain the hightest similarity(0.40) with the animeTitle Kaizoku Ouji, while using the 3rd engine the hightest similarity is obtained in in correspondence on the animeTitle \"I was king\", that contains one of the word inserted in the box (\"king\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "30\n",
      "20\n",
      "Maximum duration time Is 125\n"
     ]
    }
   ],
   "source": [
    "def finddiff(a,b):\n",
    "    res = b-a\n",
    "    if res>10:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def maxDurationTime(arr,size):\n",
    "    lists = []\n",
    "    current_max = arr[0]\n",
    "    max_last = 0\n",
    "    \n",
    "    for i in range(2, size):\n",
    "            \n",
    "            max_last = max_last + arr[i]\n",
    "            if max_last < 0:\n",
    "                max_last = 0\n",
    "            \n",
    "            \n",
    "            elif (current_max< max_last):\n",
    "                current_max = max_last\n",
    "                lists.append(arr[i])\n",
    "                \n",
    "    for i in range(0,len(lists)):\n",
    "        print(lists[i])\n",
    "            \n",
    "    return max_last\n",
    "\n",
    "\n",
    "# time array in python\n",
    "time_request = [30, 40, 25, 50, 30,20]\n",
    "size_request = len(time_request)\n",
    "# call to function to find maximum duration time \n",
    "print(\"Maximum duration time Is\" , maxDurationTime(time_request,size_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: optimal_schedule <br>\n",
    "Input: A list of requests Requests <br>\n",
    "Appointments = [] //initialize appointments list <br>\n",
    "Appointments = find_optimal(Requests, Appointments, -1,0)<br>\n",
    "Output: An optimal list of appointments Appointments<br>\n",
    "\n",
    "\n",
    "Function: find_optimal<br>\n",
    "Input: <br>\n",
    "A list of requests Requests<br>\n",
    "A list of appointments currentAppointments<br>\n",
    "Last accepted request lastRequest<br>\n",
    "Next request in the queue nextRequest<br>\n",
    "Best_schedule = [] //denotes the optimal solution found<br> \n",
    "Best_duration = 0 //denotes the max duration found<br>\n",
    "For each i = nextRequest -> Requests.length()<br>\n",
    "\t//If appointments list is empty or the last appointment and next request are not consecutive<br>\n",
    "\tIf currentAppointments.length = 0 OR lastRequest != nextRequest + 1 <br>\n",
    "\t\tAdd Requests(i) to currentAppointments  //Accept next request<br>\n",
    "\t\tD = total_duration(currentAppointments) //Evaluate current solution<br>\n",
    "\t\tIf D > Best_duration<br>\n",
    "\t\t\tBest_duration = D<br>\n",
    "\t\t\tBest_schedule =  currentAppointments<br>\n",
    "\t\t//Add next requests in queue and reevaluate (recursively)<br>\n",
    "Find_optimal(Requests, currentAppointments, i, i+1)<br>\n",
    "\tElse <br>\n",
    "\t\tFind_optimal(Requests, currentAppointments, lastRequest, i+1)<br>\n",
    "End if<br>\n",
    "End for<br>\n",
    "Output: Best_schedule<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_duration(Appointments) :\n",
    "    duration = 0\n",
    "    for a in range (len(Appointments)) :\n",
    "        duration = duration + a \n",
    "    return duration\n",
    "    \n",
    "\n",
    "def find_optimal (Requests, currentAppointments, lastRequest, nextRequest) :\n",
    "    best_schedule = []\n",
    "    best_duration = []\n",
    "\n",
    "    for i in range(nextRequest, len(Requests)) :\n",
    "        if len(currentAppointments) == 0 or lastRequest != (nextRequest + 1) :\n",
    "            currentAppointments.append(Requests[i])\n",
    "            d = total_duration(currentAppointments)\n",
    "            if (d > 0) :\n",
    "                best_duration = d\n",
    "                best_schedule.extend(currentAppointments)\n",
    "            find_optimal(Requests,currentAppointments,i,i+1)    \n",
    "\n",
    "        else :\n",
    "            find_optimal(Requests,currentAppointments,lastRequest,i+1)\n",
    "\n",
    "    return best_schedule\n",
    "\n",
    "\n",
    "def optimal_schedule (Requests) :\n",
    "    Appointments = []\n",
    "    Appointments = find_optimal(Requests, Appointments, -1 ,0)\n",
    "    return Appointments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
